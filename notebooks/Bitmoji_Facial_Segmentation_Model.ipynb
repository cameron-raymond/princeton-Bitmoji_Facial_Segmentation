{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitmoji Facial Segmentation Model\n",
    "\n",
    "[Reference 1](https://towardsdatascience.com/building-a-custom-semantic-segmentation-model-abb0843ac12d)\n",
    "\n",
    "[Reference 1 Git](https://github.com/sam-watts/futoshiki-solver)\n",
    "\n",
    "[Reference 2](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from tqdm.notebook import tqdm\n",
    "import segmentation_models_pytorch as smp\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Rescale,RandomCrop,ToTensor,FaceLandmarksDataset\n",
    "\n",
    "transformed_dataset = FaceLandmarksDataset(csv_file='../data/bitmoji_annotation.csv',\n",
    "                                           root_dir='../data/training/',\n",
    "                                           transform=transforms.Compose([Rescale((400,600)),\n",
    "                                                                         RandomCrop((320, 532)),\n",
    "                                                                         ToTensor()]))\n",
    "\n",
    "\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=1,\n",
    "                        shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image augmentation\n",
    "\n",
    "To get the most out of our labelled data we'll use the library [albumentations](https://albumentations.ai/) to add noise to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "\n",
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.ShiftScaleRotate(scale_limit=0.2, rotate_limit=20, shift_limit=0.2, p=0.8, border_mode=0),\n",
    "        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n",
    "        albu.RandomCrop(height=320, width=320, always_apply=True),\n",
    "        albu.IAAAdditiveGaussianNoise(p=0.1),\n",
    "        albu.IAAPerspective(p=0.5),\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.CLAHE(p=1),\n",
    "                albu.RandomBrightness(p=1),\n",
    "                albu.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return albu.Compose(train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "# Load basemodel\n",
    "ENCODER = 'efficientnet-b3'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['face']\n",
    "ACTIVATION = 'sigmoid'\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Facial Segmentation)",
   "language": "python",
   "name": "facial_seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
